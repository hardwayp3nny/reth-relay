use redis::{aio::MultiplexedConnection, Client, RedisResult};
use serde_json::Value as JsonValue;
use std::collections::HashSet;
use std::sync::Arc;
use tokio::sync::{mpsc, Mutex, OnceCell};
use uuid::Uuid;
use crate::analysis::TxAnalysisResult;

// Redisé”®çš„å‰ç¼€
const TX_ANALYSIS_PREFIX: &str = "tx_analysis:";
const CTF_TX_PREFIX: &str = "ctf_tx:";

// Redisé¢‘é“åç§°
pub const TX_ANALYSIS_CHANNEL: &str = "mempool:tx_analysis";
pub const CTF_TX_CHANNEL: &str = "mempool:ctf_tx";

// TTLè®¾ç½®ä¸º60ç§’ï¼ˆé¿å…æ•°æ®è¿‡æ—©è¿‡æœŸï¼‰
const TTL_SECONDS: usize = 60;

// æé€Ÿæ‰¹å¤„ç†é…ç½®
const BATCH_SIZE: usize = 100;  // å¢åŠ æ‰¹æ¬¡å¤§å°ï¼Œæé«˜ååé‡
const CHANNEL_BUFFER_SIZE: usize = 20000;  // å¢åŠ ç¼“å†²åŒº
const FAST_BATCH_INTERVAL_MS: u64 = 15;  // é™ä½åˆ°15msï¼Œæå‡å®æ—¶æ€§

#[derive(Debug, Clone)]
pub enum RedisOperation {
    TxAnalysis(Vec<TxAnalysisResult>),
    CtfTx(String),
}

#[derive(Debug)]
struct BatchedOperations {
    tx_analyses: Vec<TxAnalysisResult>,
    ctf_txs: Vec<String>,
    processed_hashes: HashSet<String>,
}

static REDIS_CLIENT: OnceCell<Client> = OnceCell::const_new();
static REDIS_SENDER: OnceCell<mpsc::UnboundedSender<RedisOperation>> = OnceCell::const_new();

/// åˆå§‹åŒ–Rediså¼‚æ­¥å¤„ç†å™¨
pub async fn init_redis_async() -> RedisResult<()> {
    let redis_url = std::env::var("REDIS_URL").unwrap_or_else(|_| "redis://127.0.0.1:6379".to_string());
    
    let client = Client::open(redis_url)?;
    
    // åˆ›å»ºå¤šè·¯å¤ç”¨è¿æ¥ï¼ˆçœŸæ­£çš„è¿æ¥æ± ï¼‰
    let conn = client.get_multiplexed_async_connection().await?;
    
    REDIS_CLIENT.set(client).map_err(|_| {
        redis::RedisError::from((redis::ErrorKind::ClientError, "Failed to set Redis client"))
    })?;
    
    // åˆ›å»ºå¼‚æ­¥å¤„ç†å™¨é€šé“
    let (tx, rx) = mpsc::unbounded_channel();
    
    REDIS_SENDER.set(tx).map_err(|_| {
        redis::RedisError::from((redis::ErrorKind::ClientError, "Failed to set Redis sender"))
    })?;
    
    // å¯åŠ¨åå°å¤„ç†å™¨
    tokio::spawn(redis_processor(conn, rx));
    
    println!("ğŸš€ Rediså¼‚æ­¥å¤„ç†å™¨å·²å¯åŠ¨");
    Ok(())
}

/// åå°Rediså¤„ç†å™¨ - æ‰¹é‡å¤„ç†æ“ä½œ
async fn redis_processor(
    conn: MultiplexedConnection,
    mut rx: mpsc::UnboundedReceiver<RedisOperation>,
) {
    let conn = Arc::new(Mutex::new(conn));
    let mut batch = BatchedOperations {
        tx_analyses: Vec::with_capacity(BATCH_SIZE),
        ctf_txs: Vec::with_capacity(BATCH_SIZE),
        processed_hashes: HashSet::new(),
    };
    
    // æé€Ÿæ‰¹å¤„ç†å®šæ—¶å™¨ï¼ˆ15msé—´éš”ï¼‰
    let mut batch_timer = tokio::time::interval(tokio::time::Duration::from_millis(FAST_BATCH_INTERVAL_MS));
    
    loop {
        tokio::select! {
            // æ¥æ”¶æ–°æ“ä½œ
            Some(operation) = rx.recv() => {
                match operation {
                    RedisOperation::TxAnalysis(analyses) => {
                        for analysis in analyses {
                            let hash = format!("{:x}", analysis.hash);
                            if !batch.processed_hashes.contains(&hash) {
                                batch.tx_analyses.push(analysis);
                                batch.processed_hashes.insert(hash);
                            }
                        }
                    }
                    RedisOperation::CtfTx(data) => {
                        // ä»JSONä¸­æå–å“ˆå¸Œ
                        if let Ok(json) = serde_json::from_str::<JsonValue>(&data) {
                            if let Some(hash) = json.get("hash").and_then(|h| h.as_str()) {
                                if !batch.processed_hashes.contains(hash) {
                                    batch.ctf_txs.push(data);
                                    batch.processed_hashes.insert(hash.to_string());
                                }
                            }
                        }
                    }
                }
                
                // æ™ºèƒ½æ‰¹å¤„ç†ï¼šæ»¡æ‰¹æ¬¡æˆ–é«˜é¢‘æ—¶ç«‹å³å¤„ç†
                let should_process_immediately = 
                    batch.tx_analyses.len() >= BATCH_SIZE || 
                    batch.ctf_txs.len() >= BATCH_SIZE ||
                    (batch.tx_analyses.len() + batch.ctf_txs.len() >= 20); // é«˜é¢‘æ¨¡å¼ï¼š20ä¸ªå°±å¤„ç†
                
                if should_process_immediately {
                    if let Err(e) = process_batch(&conn, &mut batch).await {
                        eprintln!("âŒ æ‰¹å¤„ç†å¤±è´¥: {}", e);
                    }
                }
            }
            
            // å®šæ—¶å¤„ç†æ‰¹æ¬¡ï¼ˆå³ä½¿æ²¡æ»¡ï¼‰
            _ = batch_timer.tick() => {
                if !batch.tx_analyses.is_empty() || !batch.ctf_txs.is_empty() {
                    if let Err(e) = process_batch(&conn, &mut batch).await {
                        eprintln!("âŒ å®šæ—¶æ‰¹å¤„ç†å¤±è´¥: {}", e);
                    }
                }
            }
        }
    }
}

/// å¤„ç†ä¸€ä¸ªæ‰¹æ¬¡çš„æ“ä½œ
async fn process_batch(
    conn: &Arc<Mutex<MultiplexedConnection>>,
    batch: &mut BatchedOperations,
) -> RedisResult<()> {
    if batch.tx_analyses.is_empty() && batch.ctf_txs.is_empty() {
        return Ok(());
    }
    
    let start = std::time::Instant::now();
    let mut conn = conn.lock().await;
    
    // æ„å»ºæ‰¹é‡pipeline
    let mut pipe = redis::pipe();
    let mut publish_commands = Vec::new();
    
    // å¤„ç†tx_analysisæ•°æ®
    for analysis in &batch.tx_analyses {
        let storage_key = generate_key_with_timestamp(TX_ANALYSIS_PREFIX);
        
        let mut value = serde_json::to_value(analysis).map_err(|e| {
            redis::RedisError::from((redis::ErrorKind::ClientError, "Serialization failed", e.to_string()))
        })?;
        
        if let JsonValue::Object(ref mut obj) = value {
            obj.insert(
                "ts_local".to_string(),
                JsonValue::String(chrono::Local::now().format("%Y-%m-%d %H:%M:%S%.6f").to_string()),
            );
        }
        
        let json_str = serde_json::to_string(&value).map_err(|e| {
            redis::RedisError::from((redis::ErrorKind::ClientError, "Serialization failed", e.to_string()))
        })?;
        
        // æ·»åŠ åˆ°pipelineï¼ˆä¸åšå»é‡æ£€æŸ¥ï¼Œæé«˜æ€§èƒ½ï¼‰
        pipe.cmd("SET").arg(&storage_key).arg(&json_str).arg("EX").arg(TTL_SECONDS);
        
        // å‡†å¤‡å‘å¸ƒå‘½ä»¤
        publish_commands.push((TX_ANALYSIS_CHANNEL, json_str));
    }
    
    // å¤„ç†ctf_txæ•°æ®
    for ctf_data in &batch.ctf_txs {
        let storage_key = generate_key_with_timestamp(CTF_TX_PREFIX);
        
        let mut json_value: JsonValue = serde_json::from_str(ctf_data).map_err(|e| {
            redis::RedisError::from((redis::ErrorKind::ClientError, "JSON parse failed", e.to_string()))
        })?;
        
        if let JsonValue::Object(ref mut obj) = json_value {
            obj.insert(
                "ts_local".to_string(),
                JsonValue::String(chrono::Local::now().format("%Y-%m-%d %H:%M:%S%.6f").to_string()),
            );
        }
        
        let json_str = serde_json::to_string(&json_value).map_err(|e| {
            redis::RedisError::from((redis::ErrorKind::ClientError, "Serialization failed", e.to_string()))
        })?;
        
        // æ·»åŠ åˆ°pipeline
        pipe.cmd("SET").arg(&storage_key).arg(&json_str).arg("EX").arg(TTL_SECONDS);
        
        // å‡†å¤‡å‘å¸ƒå‘½ä»¤
        publish_commands.push((CTF_TX_CHANNEL, json_str));
    }
    
    // æ‰§è¡Œæ‰¹é‡å­˜å‚¨
    if !batch.tx_analyses.is_empty() || !batch.ctf_txs.is_empty() {
        pipe.query_async::<_, ()>(&mut *conn).await?;
        
        // æ‰¹é‡å‘å¸ƒï¼ˆä½¿ç”¨å•ç‹¬çš„pipelineé¿å…é˜»å¡ï¼‰
        let mut publish_pipe = redis::pipe();
        for (channel, data) in publish_commands {
            publish_pipe.cmd("PUBLISH").arg(channel).arg(data);
        }
        publish_pipe.query_async::<_, ()>(&mut *conn).await?;
    }
    
    let duration = start.elapsed();
    let total_items = batch.tx_analyses.len() + batch.ctf_txs.len();
    let duration_us = duration.as_micros();
    
    // æ€§èƒ½ç»Ÿè®¡ï¼ˆä»…åœ¨æœ‰æ•°æ®æ—¶è¾“å‡ºï¼‰
    if total_items > 0 {
        println!(
            "âš¡ æé€Ÿæ‰¹å¤„ç†: {} tx + {} ctf = {} items, {}Î¼s ({}k items/s)",
            batch.tx_analyses.len(),
            batch.ctf_txs.len(),
            total_items,
            duration_us,
            if duration_us > 0 { (total_items as u128 * 1_000_000) / duration_us } else { 0 }
        );
        
        // æ€§èƒ½å‘Šè­¦
        if duration_us > 50_000 {
            eprintln!("âš ï¸ Redisæ‰¹å¤„ç†å»¶è¿Ÿè¿‡é«˜: {}ms", duration_us / 1000);
        }
    }
    
    // æ¸…ç©ºæ‰¹æ¬¡
    batch.tx_analyses.clear();
    batch.ctf_txs.clear();
    batch.processed_hashes.clear();
    
    Ok(())
}

/// ç”Ÿæˆå¸¦æ—¶é—´æˆ³çš„å”¯ä¸€é”®ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰
fn generate_key_with_timestamp(prefix: &str) -> String {
    let timestamp = std::time::SystemTime::now()
        .duration_since(std::time::UNIX_EPOCH)
        .unwrap()
        .as_millis();
    let uuid = Uuid::new_v4().simple(); // ä½¿ç”¨simpleæ ¼å¼ï¼Œæ›´çŸ­
    format!("{}{}__{}", prefix, timestamp, uuid)
}

/// å¼‚æ­¥å­˜å‚¨äº¤æ˜“åˆ†æç»“æœï¼ˆéé˜»å¡ï¼‰
pub fn store_tx_analysis_async(results: Vec<TxAnalysisResult>) -> Result<(), &'static str> {
    if results.is_empty() {
        return Ok(());
    }
    
    if let Some(sender) = REDIS_SENDER.get() {
        if let Err(_) = sender.send(RedisOperation::TxAnalysis(results)) {
            return Err("Redis channelå·²å…³é—­");
        }
        Ok(())
    } else {
        Err("Redisæœªåˆå§‹åŒ–")
    }
}

/// å¼‚æ­¥å­˜å‚¨CTFäº¤æ˜“ï¼ˆéé˜»å¡ï¼‰
pub fn store_ctf_tx_async(data: String) -> Result<(), &'static str> {
    if let Some(sender) = REDIS_SENDER.get() {
        if let Err(_) = sender.send(RedisOperation::CtfTx(data)) {
            return Err("Redis channelå·²å…³é—­");
        }
        Ok(())
    } else {
        Err("Redisæœªåˆå§‹åŒ–")
    }
}

/// è·å–æ€§èƒ½ç»Ÿè®¡ï¼ˆå¯é€‰å®ç°ï¼‰
pub async fn get_performance_stats() -> RedisResult<String> {
    // å¯ä»¥æ·»åŠ æ€§èƒ½ç›‘æ§ç»Ÿè®¡
    Ok("æ€§èƒ½ç»Ÿè®¡åŠŸèƒ½å¾…å®ç°".to_string())
}
